[
  {
    "path": "posts/2022-03-23-population-genetics-models/",
    "title": "Population Genetics Models",
    "description": "In this post, we introduce population genetics models and the Hardy-Weinberg theory.",
    "author": [
      {
        "name": "Jason M. Graham",
        "url": {}
      }
    ],
    "date": "2022-03-23",
    "categories": [],
    "contents": "\nIntroduction\nOver the course of the Topics in Biomathematics notes, we\nhave seen many applications of biomathematical models and techniques\nrelated to population dynamics. However, the models we have looked at\nlargely ignore evolutionary phenomena. In this post, we briefly describe\nsome of the introductory mathematical models for population genetics\nwhich is a path of entry to incorporating biological evolution into the\nstudy of populations. Our goal is to provide an point of entry to an\narea of biomathematics that is not covered in the primary lectures and\nnotes. Further, this topic provides an simple exampl eof the use of\nprobability in biomathematics.\nThe treatment here closely follows Chapter 14 from Mathematics for\nthe Life Sciences by Bodine et al. Further material of using\nmore advanced mathematics may be found in Section 3.7 of An\nIntroduction to Mathematical Biology by Allen or Chapter 4 from\nEssential Mathematical Biology by Britton.\nBackground\nRecall that genetics is concerned\nwith genes which are\nthe basic biological unit of heredity. Population genetics studies how\nthe genetics of a population changes over time. That is, population\ngenetics tracks genes from one generation to the next. An allele is a variant of a\ngiven gene and the frequencies of alleles that occur in a population is\nan important variable in population genetics models.\nSuppose that we have two alleles denoted by \\(A\\) and \\(a\\). We think of \\(A\\) as the dominant allele and \\(a\\) as the recessive allele. A human with\ntwo sets of chromosomes could\nhave one of any of the three allele combinations:\n\\(AA\\)\n\\(Aa\\)\n\\(aa\\)\nHardy-Weinberg Equilibrium\nLet \\(P_{A}\\) denote the frequency\nof allele \\(A\\) in a population and\n\\(P_{a}\\) denote the frequency of\nallele \\(a\\) in a population. We view\nthese frequency values as probabilities and thus must have\n\\(P_{A} + P_{a} = 1\\)\nsince the sum of all possible probability values must add to 1.\nSuppose that we want to track genetics across a single generation. We\nwould compute the following frequencies:\n\\(P_{AA}\\) - frequency of \\(AA\\) genotype in the population,\n\\(P_{Aa}\\) - frequency of \\(Aa\\) genotype in the population,\nand\n\\(P_{aa}\\) - frequency of \\(aa\\) genotype in the population.\nViewing these frequencies as probabilities then leads to\n\\(P_{AA} + P_{Aa} + P_{aa} = 1\\)\nbecause those three options exhaust all possibilities.\nWe want to relate the values \\(P_{AA}\\), \\(P_{Aa}\\), \\(P_{aa}\\), and \\(P_{A}\\) and \\(P_{a}\\). The following are assumed ot\nhold:\nMating is random,\nthere is no variation in the number of progeny from parents of\ndifferent genotypes,\nall genotypes are equal with regard to fitness, and\nthere are no mutations.\nThen the Hardy-Weinberg\nlaw states that gene frequencies and allele frequencies do\nnot vary from one generation to the next. If \\(N\\) is the total population size, then\n\\[\n\\begin{align}\nP_{A} &= \\frac{\\text{total number of A alleles}}{2N} \\\\\nP_{a} &= \\frac{\\text{total number of a alleles}}{2N}\n\\end{align}\n\\]\nG.H. Hardy was a\nprominant mathematician of the time and Wilhelm\nWeinberg was an obstetrician-gynecologist. The two independently\ndeveloped the ideas known as the Hardy-Weinberg law. However, others had\ndeveloped previously developed soem special cases of the Hardy-Weinberg\nlaw.\nThen, by Hardy-Weinberg,\n\\[\n\\begin{align}\nP_{A} &= \\frac{2 N P_{AA} + N P_{Aa}}{2N} = P_{AA} + \\frac{1}{2}\nP_{Aa}, \\\\\nP_{a} &= \\frac{2 N P_{aa} + N P_{Aa}}{2N} = P_{aa} + \\frac{1}{2}\nP_{Aa}\n\\end{align}\n\\]\nNote that we have\n\\(P_{A} + P_{a} = P_{AA} + \\frac{1}{2}\nP_{Aa} + P_{aa} + \\frac{1}{2} P_{Aa} = 1\\).\nGiven that we know the genotype frequencies in the current\ngeneration, how can we find the genotype frequencies in the following\ngeneration? Making use of our previous assumptions which allows us to\nconclude independence,\nwe have that\n\\[\n\\begin{align}\n\\text{next generation frequency of AA} = Q_{AA} = P_{A}P_{A} = P_{A}^2\n\\\\\n\\text{next generation frequency of Aa} = Q_{Aa} = P_{Aa} + P_{Aa} =\n2P_{Aa} = 2P_{A}P_{a} \\\\\n\\text{next generation frequency of aa} = Q_{aa} = P_{a}P_{a} = P_{a}^2\n\\end{align}\n\\] Now observe that\n\\(Q_{AA} + Q_{Aa} + Q_{aa} = P_{A}^2 +\n2P_{A}P_{a} + P_{a}^2 = (P_{A}+P_{a})^2 = 1\\)\nFrom this, we have\n\\[\n\\begin{align}\nQ_{A} &= Q{AA} + \\frac{1}{2}Q_{Aa} \\\\\n      &=  P_{A}^2 + \\frac{1}{2}2P_{A}P_{a} \\\\\n      &=  P_{A}^2 + P_{A}P_{a} \\\\\n      &= P_{A}(P_{A} + P_{a}) \\\\\n      &= P_{A} \\cdot 1 \\\\\n      &= P_{A}\n\\end{align}\n\\]\nand similarly\n\\[\n\\begin{align}\nQ_{a} &= Q_{aa} + \\frac{1}{2}Q_{Aa} \\\\\n      &= P_{a}^2 + \\frac{1}{2}2P_{A}P_{a} \\\\\n      &=  P_{a}^2 + P_{A}P_{a} \\\\\n      &= P_{a}(P_{a} + P_{A}) \\\\\n      &= P_{a} \\cdot 1 \\\\\n      &= P_{a}\n\\end{align}\n\\] and thus we have that in the next generation\n\\(Q_{A} + Q_{a} = P_{A} + P_{a} =\n1\\)\nHardy-Weinberg Selection\nModel\nSuppose that survival is a function of genotype and let \\(S=\\)probability of survival to reproductive\nage. Define\n\\(s_{AA}=P(S|AA)\\),\n\\(s_{Aa}=P(S,Aa)\\),\n\\(s_{aa}=P(S|aa)\\)\nRecall that \\(P(X|Y)\\) denotes a conditional\nprobability. By the law of\ntotal probability we must have\n\\[\n\\begin{align}\nP(S) &= P(S|AA)P(AA) + P(S|Aa)P(Aa) + P(S|aa)P(aa) \\\\\n&= s_{AA}P_{A}^2 + 2s_{Aa}P_{A}P_{a} + s_{aa}P_{a}^2\n\\end{align}\n\\] Then, Bayes’ theorem\nimplies\n\\[\n\\begin{align}\nP(AA|S) &= \\frac{P(S|AA)P(AA)}{P(S)} =\n\\frac{s_{AA}P_{A}^2}{s_{AA}P_{A}^2 + 2s_{Aa}P_{A}P_{a} + s_{aa}P_{a}^2}\n\\\\\nP(Aa|S) &= \\frac{P(S|Aa)P(Aa)}{P(S)} =\n\\frac{2s_{A2}P_{A}P_{a}}{s_{AA}P_{A}^2 + 2s_{Aa}P_{A}P_{a} +\ns_{aa}P_{a}^2} \\\\\nP(aa|S) &= \\frac{P(S|aa)P(aa)}{P(S)} =\n\\frac{s_{AA}P_{a}^2}{s_{AA}P_{A}^2 + 2s_{Aa}P_{A}P_{a} + s_{aa}P_{a}^2}\n\\end{align}\n\\]\nNow, for an initial generation, denote \\(p_{0}=P_{A}\\) and \\(q_{0}=P_{a}\\) in the next generation we\nwill have\n\\[\n\\begin{align}\np_{1} &= P(AA|S) + \\frac{1}{2}P(Aa|S) \\\\\n&= \\frac{s_{AA}P_{A}^2}{s_{AA}P_{A}^2 + 2s_{Aa}P_{A}P_{a} +\ns_{aa}P_{a}^2} + \\frac{1}{2}\\frac{2s_{Aa}P_{A}P_{a}}{s_{AA}P_{A}^2 +\n2s_{Aa}P_{A}P_{a} + s_{aa}P_{a}^2} \\\\\n&= \\frac{s_{AA}p_{0}^2+s_{Aa}p_{0}q_{0}}{s_{AA}p_{0}^2 +\n2s_{Aa}p_{0}q_{0} + s_{aa}q_{0}^2} \\\\\n&= p_{0} \\frac{s_{AA}p_{0} + s_{Aa}q_{0}}{s_{AA}p_{0}^2 +\n2s_{Aa}p_{0}q_{0} + s_{aa}q_{0}^2}\n\\end{align}\n\\]\nand\n\\[\n\\begin{align}\nq_{1} &= P(aa|S) + \\frac{1}{2}P(Aa|S) \\\\\n&= \\frac{s_{aa}P_{a}^2}{s_{AA}P_{A}^2 + 2s_{Aa}P_{A}P_{a} +\ns_{aa}P_{a}^2} + \\frac{1}{2}\\frac{2s_{Aa}P_{A}P_{a}}{s_{AA}P_{A}^2 +\n2s_{Aa}P_{A}P_{a} + s_{aa}P_{a}^2} \\\\\n&= \\frac{s_{aa}q_{0}^2+s_{Aa}p_{0}q_{0}}{s_{AA}p_{0}^2 +\n2s_{Aa}p_{0}q_{0} + s_{aa}q_{0}^2} \\\\\n&= q_{0} \\frac{s_{aa}q_{0} + s_{Aa}p_{0}}{s_{AA}p_{0}^2 +\n2s_{Aa}p_{0}q_{0} + s_{aa}q_{0}^2}\n\\end{align}\n\\]\nMore generally, from generation \\(n\\) to generation \\(n+1\\) we have\n\\[\n\\begin{align}\np_{n+1} &= p_{n} \\frac{s_{AA}p_{n} + s_{Aa}q_{n}}{s_{AA}p_{n}^2 +\n2s_{Aa}p_{n}q_{n} + s_{aa}q_{n}^2} \\\\\nq_{n+1} &= q_{n} \\frac{s_{aa}q_{n} + s_{Aa}p_{n}}{s_{AA}p_{n}^2 +\n2s_{Aa}p_{n}q_{n} + s_{aa}q_{n}^2}\n\\end{align}\n\\]\nwhich is a system of nonlinear difference equations.\nConclusion\nWe have introduced the beginnings of population genetics and the\nHardy-Weinberg theory. Further, we have seen how basic probability\ntheory combines with this to derive difference equation models. To\nexplore this topic further, see the references suggested at the\nbeginning of the post.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-03-24T08:43:21-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-02-21-non-dimensionalizing-the-chemostat-model/",
    "title": "Non-Dimensionalizing the Chemostat Model",
    "description": "We apply our techniques of dimensional analysis to non-dimesionalize the chemostat model equations.",
    "author": [
      {
        "name": "Jason M. Graham",
        "url": {}
      }
    ],
    "date": "2022-02-21",
    "categories": [],
    "contents": "\nBackground\nWe will work with the chemostat model derived in notes\n7:\n\\[\n\\begin{align}\n\\frac{dN}{dt} &=  G(C)N - \\frac{F}{V}N \\\\\n\\frac{dC}{dt} &= -\\alpha G(C)N - \\frac{F}{V}C + \\frac{F}{V}C_{0}\n\\end{align}\n\\]\nObserve the following, in terms of unit dimensions, we must have\n\\(\\left[G(C) \\right] =\n\\frac{1}{T}\\),\n\\(\\left[\\frac{F}{V} \\right] =\n\\frac{1}{T}\\),\n\\([\\alpha] = \\frac{[C]}{[N]}\\),\nand\n\\([C_{0}] = [C]\\)\nConsider the case where we choose a Michaelis-Menten rate law for\n\\(G(C)\\) as in notes 8.\nThat is, take\n\\(G(C) = \\frac{K_{\\text{max}}C}{k_{n} +\nC}\\)\nNote that this requires\n\\([K_{\\text{max}}] =\n\\frac{1}{T}\\), and\n\\([k_{n}] = [C]\\)\nThus, our model equations are\n\\[\n\\begin{align}\n\\frac{dN}{dt} &=  \\frac{K_{\\text{max}}C}{k_{n} + C}N - \\frac{F}{V}N\n\\\\\n\\frac{dC}{dt} &= -\\alpha \\frac{K_{\\text{max}}C}{k_{n} + C}  N -\n\\frac{F}{V}C + \\frac{F}{V}C_{0}\n\\end{align}\n\\] and these are the equations we will non-dimensionalize.\nNon-Dimensionalization\nWe will carry out a generic non-dimensionalization as described in notes 10.\nLet \\(N^{\\ast}\\) and \\(C^{\\ast}\\) be characteristic scales for\nconcentrations and let \\(t^{\\ast}\\) be\na characteristic time scale. Then, define \\(x=\\frac{N}{N^{\\ast}}\\), \\(y=\\frac{C}{C^{\\ast}}\\), and \\(\\tau=\\frac{t}{t^{\\ast}}\\). Explicit\nexpressions for the scales \\(N^{\\ast}\\), \\(C^{\\ast}\\), and \\(t^{\\ast}\\) will be determined later.\nBy the chain rule we have\n\\[\n\\begin{align}\n\\frac{dx}{d\\tau} &= \\frac{dx}{dt}\\frac{dt}{d\\tau} =\nt^{\\ast}\\frac{d}{dt}\\frac{N}{N^{\\ast}}\\\\\n\\frac{dy}{d\\tau} &= \\frac{dy}{dt}\\frac{dt}{d\\tau} =\nt^{\\ast}\\frac{d}{dt}\\frac{C}{C^{\\ast}}\n\\end{align}\n\\] which implies that\n\\[\n\\begin{align}\n\\frac{dx}{d\\tau} &= \\frac{t^{\\ast}}{N^{\\ast}}\\left(\n\\frac{K_{\\text{max}}C}{k_{n} + C}N - \\frac{F}{V}N\\right)\\\\\n\\frac{dy}{d\\tau} &= \\frac{t^{\\ast}}{C^{\\ast}}\\left(-\\alpha\n\\frac{K_{\\text{max}}C}{k_{n} + C}  N - \\frac{F}{V}C + \\frac{F}{V}C_{0}\n\\right)\n\\end{align}\n\\] or equivalently\n\\[\n\\begin{align}\n\\frac{dx}{d\\tau} &= t^{\\ast} \\frac{K_{\\text{max}}C^{\\ast}y}{k_{n} +\nC^{\\ast}y} x - t^{\\ast} \\frac{F}{V} x \\\\\n\\frac{dy}{d\\tau} &= -\\alpha \\frac{t^{\\ast}}{C^{\\ast}}\n\\frac{K_{\\text{max}}C^{\\ast}y}{k_{n} + C^{\\ast}y}  N^{\\ast}x -\nt^{\\ast}\\frac{F}{V}y + \\frac{t^{\\ast}}{C^{\\ast}} \\frac{F}{V}C_{0}\n\\end{align}\n\\]\nNow we need to choose expressions for the scale factors \\(N^{\\ast}\\), \\(C^{\\ast}\\), and \\(t^{\\ast}\\). Based on our earlier\ndimensional analysis, it is reasonable to choose\n\\(t^{\\ast}=\\frac{V}{F}\\),\nand\n\\(C^{\\ast}=k_{n}\\).\nDoing so leads to\n\\[\n\\begin{align}\n\\frac{dx}{d\\tau} &= \\frac{V}{F} \\frac{K_{\\text{max}}k_{n}y}{k_{n} +\nk_{n}y} x - \\frac{V}{F} \\frac{F}{V} x \\\\\n\\frac{dy}{d\\tau} &= -\\alpha \\frac{\\frac{V}{F}}{k_{n}}\n\\frac{K_{\\text{max}}k_{n}y}{k_{n} + k_{n}y}  N^{\\ast}x -\n\\frac{V}{F}\\frac{F}{V}y + \\frac{\\frac{V}{F}}{k_{n}} \\frac{F}{V}C_{0}\n\\end{align}\n\\]\nor equivalently,\n\\[\n\\begin{align}\n\\frac{dx}{d\\tau} &= \\frac{V K_{\\text{max}}}{F} \\frac{y}{1 + y} x\n-  x \\\\\n\\frac{dy}{d\\tau} &= - \\frac{\\alpha V K_{\\text{max}}}{F k_{n}}\nN^{\\ast} \\frac{y}{1 + y} x - y + \\frac{C_{0}}{k_{n}}\n\\end{align}\n\\] It remains to choose an expression for \\(N^{\\ast}\\). To do this, we examine the unit\ndimensions for \\(\\frac{\\alpha V\nK_{\\text{max}}}{F k_{n}}\\). We have\n\\[\n\\left[\\frac{\\alpha V K_{\\text{max}}}{F k_{n}} \\right] = \\left[\\alpha\n\\right]\\left[\\frac{V}{F} \\right]\\left[\\frac{K_{\\text{max}}}{k_{n}}\n\\right] = \\frac{[C]}{[N]} T \\frac{\\frac{1}{T}}{[C]} = \\frac{1}{[N]}\n\\]\nThus, we may take\n\\[\nN^{\\ast} = \\frac{F k_{n}}{\\alpha V K_{\\text{max}}}\n\\]\nwhich gives\n\\[\n\\begin{align}\n\\frac{dx}{d\\tau} &= a_{1} \\frac{y}{1 + y} x -  x \\\\\n\\frac{dy}{d\\tau} &= - \\frac{y}{1 + y} x - y + a_{2}\n\\end{align}\n\\]\nwhere \\(a_{1} = \\frac{V\nK_{\\text{max}}}{F}\\) and \\(a_{2} =\n\\frac{C_{0}}{k_{n}}\\) are dimensionless quantities.\nConclusion\nA non-dimensional version of the chemostat model equations is\n\\[\n\\begin{align}\n\\frac{dx}{d\\tau} &= a_{1} \\frac{y}{1 + y} x -  x \\\\\n\\frac{dy}{d\\tau} &= - \\frac{y}{1 + y} x - y + a_{2}\n\\end{align}\n\\]\nwith \\(a_{1} = \\frac{V\nK_{\\text{max}}}{F}\\) and \\(a_{2} =\n\\frac{C_{0}}{k_{n}}\\). Notice that we have reduced a model\nexpression involving six parameters to an equivalent expression with\nonly two parameters.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-02-21T09:16:00-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-02-04-fitting-a-line/",
    "title": "Fitting a Line",
    "description": "How to fit a line to data in R.",
    "author": [
      {
        "name": "Jason M. Graham",
        "url": {}
      }
    ],
    "date": "2022-02-04",
    "categories": [],
    "contents": "\nBackground\nIn section 8\nof the course notes, we derived the Michaelis-Menten\nfunctional form:\n\\[ f(c) = K_{\\text{max}}\\frac{c}{k_{n} +\nc}, \\]\nand stated that we can estimate the parameters \\(K_{\\text{max}}\\) and \\(k_{n}\\) using the linear equation\n\\[\\frac{1}{f(c)} =\n\\frac{k_{n}}{K_{\\text{max}}}\\frac{1}{c} +\n\\frac{1}{K_{\\text{max}}}.\\] This post discusses the practical\naspects of fitting a line to data in R. This is the same as estimating\nthe slope and intercept parameters in an equation for a line given data\npoints that you want the line to “pass through.”\nLine Fitting\nSuppose we have some number \\(n\\)\ndata points \\((x_{1},y_{1})\\), \\((x_{2},y_{2})\\), \\(\\ldots\\), \\((x_{n},y_{n})\\). If these data points all\nlie on the same line, then it is trivial to fit a line through them\nbecause any two colinear points uniquely determine a line, and it’s\nequation is found in the usual pre-calculus way using the formula for\nslope and intercept.\nHowever, real data never falls exactly on a perfect line even if the\nunderlying relationship is linear. There is always measurement error or\nnoise associated with real data. Suppose our data looks as follows:\n\n\nShow code\n\nset.seed(1234)\nx <- sort(runif(15,0,4))\ny <- 2*x + 3 + rnorm(15,0.25)\nmy_data <- tibble(x=x,y=y)\nmy_data %>% \n  ggplot(aes(x=x,y=y)) + \n  geom_point() +  \n  theme(text = element_text(size = 15))\n\n\n\n\nIt is not unreasonable to try to model this data with a linear\nrelationship. In fact, here is the “best-fit” line for this data:\n\n\nShow code\n\nmy_data %>% \n  ggplot(aes(x=x,y=y)) + \n  geom_point() +  \n  geom_smooth(method=\"lm\",se=FALSE) + \n  theme(text = element_text(size = 15))\n\n\n\n\nWhat we want to explain is how this line is determined. Every\n(non-vertical) line in the plan is determined by two parameters, the\nslope \\(a_{1}\\) and the intercept \\(a_{0}\\). The best-fit line is the one that\nminimizes the sum of squares errors (SSE)\n\\[\\text{SSE}=\\sum_{i=1}^{n}(y_{i}-\\hat{y}_{i})^2,\\]\nwhere \\(\\hat{y}_{i}=a_{1}x_{i}+a_{0}\\). The point\nis that we can think of the sum of square errors as a function of the\ntwo variables \\(a_0\\) and \\(a_1\\). Then, the best-fit line is the one\nwith the slope and intercept values that minimizes the SSE. We can\n(numerically) solve a minimization problem in R using the\noptim function in R.\n\nThere are also a number of other functions and packages available for\nsolving optimization problems in R. Which one to use depends on your\nparticular application. We use optim because it is the\nprimary option in base R.\nHere is the relevant R code:\n\n\n# define function to compute SSE\nSSE <- function(A,data=my_data){\n  y_hat <- A[1] + A[2]*data$x\n  sse <- sum((data$y-y_hat)^2) \n}\n# call optim to minimize SSE, requires an initial guess\n(parms <- optim(c(1,1),SSE)$par)\n\n\n[1] 2.829051 2.171718\n\nThis tells us that our data is best modeled by a linear equation with\nslope 2.171718 and intercept 2.8290509. Let’s see this in another\nplot:\n\n\n\nHere the dashed black line is the line with slope 2.171718 and\nintercept 2.8290509. It perfectly matches what we previously stated was\nthe best fit line.\nAnother approach to finding the slope and intercept parameters for a\nbest-fit line in R is by using the lm (short for linear\nmodel) function in R as follows:\n\n\nlm(y~x,data=my_data)$coefficients\n\n\n(Intercept)           x \n   2.828567    2.172016 \n\nNotice that we obtain the same answer as before.\nParameter Estimation\nOur discussion so far provides a very simple example of the general\nand common problem of parameter estimation. That is, given data and a\nmathematical model, what are the parameter values for the model that\nbest fit the data. To solve this problem one tries to do the same thing\nthat we have done here with our best-fit line. That is, minimize the\nerror as a function of the parameter values. The problem is that as the\nmathematical model we are trying to use becomes more complication, say\nfor example a large system of differential equations, the more nonlinear\nthe error becomes as a function of the parameters. Minimizing highly\nnonlinear functions is difficult because there may be multiple and even\nmany local minimum where the optimization algorithm can get stuck. Thus,\nparameter estimation in biomathematics is an active area of current\nresearch.\n\n\n\n",
    "preview": "posts/2022-02-04-fitting-a-line/fitting-a-line_files/figure-html5/linear-data-1.png",
    "last_modified": "2022-02-04T18:06:36-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-28-linear-stability-analysis/",
    "title": "Linear Stability Analysis",
    "description": "A short derivation of the linear stability analysis criterion.",
    "author": [
      {
        "name": "Jason M. Graham",
        "url": {}
      }
    ],
    "date": "2022-01-28",
    "categories": [],
    "contents": "\nOverview\nIn section 5\nof our notes, we discussed an analytic criterion for the stability of a\nsteady-state (or equilibrium) \\(x^{\\ast}\\) corresponding to a\none-dimensional autonomous system \\(\\frac{dx}{dt}=f(x)\\). This states that if\n\\(f\\) is a continuously differentiable\nfunction at \\(x^{\\ast}\\), then\n\\(f'(x^{\\ast}) < 0\\)\nimplies \\(x^{\\ast}\\) is stable,\nwhile\n\\(f'(x^{\\ast}) > 0\\)\nimplies \\(x^{\\ast}\\) is\nunstable.\nIn this post, we provide an explanation for the above stated\nfact.\nLinear Stability Analysis\nConsider the effect of a small perturbation to an equilibrium. Let\n\\(\\xi(t) = x(t) - x^{\\ast}\\) be a small\nperturbation away from \\(x^{\\ast}\\).\nThen by taking the derivative with respect to \\(t\\) on both sides of the equality, we see\nthat\n\\[\\begin{align}\n\\frac{d}{dt}\\xi &= \\frac{d}{dt}(x - x^{\\ast}) \\\\\n&= \\frac{d}{dt}x - \\frac{d}{dt}x^{\\ast} \\\\\n&= \\frac{dx}{dt} - 0 \\\\\n&= f(x) \\\\\n&= f(\\xi + x^{\\ast})\n\\end{align}\\]\nand therefore\n\\(\\frac{d\\xi}{dt}=f(\\xi +\nx^{\\ast})\\)\nNow Taylor’s\nformula leads to\n\\(f(\\xi + x^{\\ast}) = f(x^{\\ast}) +\nf'(x^{\\ast})\\xi + \\mathcal{O}(\\xi^2)\\)\nand thus\n\\(\\frac{d\\xi}{dt} \\approx\nf'(x^{\\ast})\\xi\\)\nwhere we have used the fact that \\(f(x^{\\ast})=0\\).\nRecall that a first-order linear system has the form\n\\(\\frac{dx}{dt}=ax\\)\nand the general solution to such a linear system is \\(x(t)=Ce^{at}\\), where \\(C\\) is a constant. Thus, we see that a\nsolution to a first-order linear system will asymptotically decay if the\ncoefficient \\(a\\) is negative, but grow\nif the coefficient \\(a\\) is\npositive.\nNow, observe that, based on our derivation, the differential equation\nfor a small perturbation \\(\\xi(t)\\)\nsuch as we have defined it very nearly satisfies a first-order linear\nsystem with coefficient \\(f'(x^{\\ast})\\). So, we expect that the\neffect of a perturbation will asymptotically decay if \\(f'(x^{\\ast}) < 0\\) but\nasymptotically increase if \\(f'(x^{\\ast})\n> 0\\). This helps to explain our criterion for stability.\nIt is important to note that our criterion provides no information\nwhen \\(f'(x^{\\ast})=0\\). In fact,\none can easily construct examples of systems that have a stable\nequilibrium such that \\(f'(x^{\\ast})=0\\) and also systems with\nan equilibrium satisfying \\(f'(x^{\\ast})=0\\) that is not stable. It\nis instructive to quickly draw the phase lines for each of the following\nsystems:\n\\(\\frac{dx}{dt}=-x^2\\),\nand\n\\(\\frac{dx}{dt}=-x^3\\).\nOne can conduct a more careful analysis of stability than what we\nhave done here. However, this requires more advanced mathematics than is\nappropriate in the context of the Topics in Biomathematics\ncourse.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-01-28T17:09:35-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-28-on-root-finding/",
    "title": "On Root Finding",
    "description": "Brief discussion on root finding in R.",
    "author": [
      {
        "name": "Jason M. Graham",
        "url": {}
      }
    ],
    "date": "2022-01-28",
    "categories": [],
    "contents": "\nOverview\nA common problem is the following:\n\nGiven a continuous function \\(f\\)\ndefined on the real numbers, find values such that \\(f(x)=0\\).\n\nThis is called the root finding problem. It comes up, for example, in\nfinding the equilibrium or steady-state solutions for a one-dimensional\nautonomous system. Note that the roots of a function \\(y=f(x)\\) coincide exactly with the \\(x\\)-intercepts of the graph of \\(y=f(x)\\). In general, when \\(f\\) is a nonlinear function, the root\nfinding problem is intractable in terms of finding exact, closed-form\nsolutions. However, algorithms for finding approximate solutions to the\nroot finding problem is a major theme in numerical\nanalysis.\nIn this post, we discuss practical numerical root-finding by way of\nthe R package rootSolve.\nThe documentation for this package is available here.\nPractical Root-Finding\nSuppose we would like to find the equilibria for the one-dimensional\nautonomous system\n\\(\\frac{dx}{dt}=f(x) = x^2 - 1\\)\nWhile it’s obvious what the answer is, let us address the problem\ncomputationally for illustrative purposes. We begin by plotting the\nfunction \\(y=f(x)\\) in order to check\nfor the existence of roots “by inspection.”\n\n\n\nThe graph tells us that there are two roots in the interval \\([-2,2]\\). Let’s call the function\nuniroot.all from rootSolve:\n\n\nuniroot.all(function(x){x^2-1}, c(-2, 2))\n\n\n[1] -1  1\n\nNotice that this in fact returns the roots for \\(f(x)=x^2-1\\).\nWhat uniroot.all attempts to do is to find all of the\nroots in the specified interval. Thus, this provides a handy tool for\nfinding equillbria for one-dimensional autonomous systems.\nThe rootSolve package is by no means the only option for\nnumerical root-finding in R. We choose to highlight it as it is\ndeveloped by the group that developed the deSolve package\nfor the numerical approximation for differential equations.\n\n\n\n",
    "preview": "posts/2022-01-28-on-root-finding/on-root-finding_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2022-02-04T18:12:28-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-01-25-course-intro/",
    "title": "Course Blog Intro",
    "description": "A short post about the course blog for MATH 463 Topics in Biomathematics.",
    "author": [
      {
        "name": "Jason M. Graham",
        "url": {}
      }
    ],
    "date": "2022-01-25",
    "categories": [],
    "contents": "\nIn addition to the other components of the Topics in Biomathematics\nwebsite, one can find links to a series of short blog posts. The\ngoal of these posts is to share small bits of information that come up\nover the span of teaching MATH 463 (beginning in Spring 2022). For\nexample, the posts may include helpful pointers to further information\non topics discussed in the class, or may even contain short\npresentations of material that is slightly beyond the official content\nof the course.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-01-25T16:28:04-05:00",
    "input_file": {}
  }
]
